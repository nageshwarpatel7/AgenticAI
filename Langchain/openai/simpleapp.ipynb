{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4f71f04",
   "metadata": {},
   "source": [
    "## Simple Gen AI App Using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8564e9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['PPLX_API_KEY'] = os.getenv('PPLX_API_KEY')\n",
    "os.environ['GEMINI_API_KEY'] = os.getenv('GEMINI_API_KEY')\n",
    "# Langsmith Tracking\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv('LANGCHAIN_PROJECT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c8b2248",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Ingestion -- from the website we need to scrape the data\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a83ad924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x25121072930>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\"https://docs.langchain.com/langsmith/observability-quickstart\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa0aa449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='Tracing quickstart - Docs by LangChainOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KLangSmithPlatform for LLM observability and evaluationOverviewQuickstartsTrace an applicationEvaluate an applicationTest promptsAPI & SDKsAPI referencePython SDKJS/TS SDKPricingPlansPricing FAQOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KAsk AIForumForumSearch...NavigationQuickstartsTracing quickstartGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationForumOn this pageGet started1. Install Dependencies2. Create an API key3. Set up environment variables4. Define your application5. Trace OpenAI calls6. Trace entire applicationNext stepsQuickstartsTracing quickstartCopy pageCopy pageObservability is important for any software application, but especially so for LLM applications. LLMs are non-deterministic by nature, meaning they can produce unexpected results. This makes them trickier than normal to debug.\\nThis is where LangSmith can help! LangSmith gives you visibility into each step your application takes when handling a request — helping you debug faster and gain confidence in your app. From prototyping to production, LangSmith has you covered with tracing,  filtering, charting, and alerting to keep your application reliable at scale.\\n\\u200bGet started\\nThis tutorial will show you how to instrument a simple RAG application that consists of a retrieval step to fetch data and an LLM call to OpenAI to answer the user question based on the data.\\nIf you’re building an application with LangChain or LangGraph, you can enable LangSmith tracing with a single environment variable.Get started by reading the guides for tracing with LangChain or tracing with LangGraph.\\n\\u200b1. Install Dependencies\\nPythonTypeScriptCopyAsk AIpip install -U langsmith openai\\n\\n\\u200b2. Create an API key\\nTo create an API key head to the LangSmith settings page. Then click + API Key.\\n\\u200b3. Set up environment variables\\nThis example uses OpenAI, but you can adapt it to use any LLM provider.\\nIf you’re using Anthropic, use the Anthropic wrapper to trace your calls. For other providers, use the traceable wrapper.\\nCopyAsk AIexport LANGSMITH_TRACING=true\\nexport LANGSMITH_API_KEY=\"<your-langsmith-api-key>\"\\nexport OPENAI_API_KEY=\"<your-openai-api-key>\"\\n\\n\\u200b4. Define your application\\nWe will instrument a simple RAG application for this tutorial, but feel free to use your own code if you’d like - just make sure it has an LLM call!\\nApplication CodePythonTypeScriptCopyAsk AIfrom openai import OpenAI\\nopenai_client = OpenAI()\\n\\n# This is the retriever we will use in RAG\\n# This is mocked out, but it could be anything we want\\ndef retriever(query: str):\\n    results = [\"Harrison worked at Kensho\"]\\n    return results\\n\\n# This is the end-to-end RAG chain.\\n# It does a retrieval step then calls OpenAI\\ndef rag(question):\\n    docs = retriever(question)\\n    system_message = \"\"\"Answer the users question using only the provided information below:\\n        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))\\n        \\n    return openai_client.chat.completions.create(\\n        messages=[\\n            {\"role\": \"system\", \"content\": system_message},\\n            {\"role\": \"user\", \"content\": question},\\n        ],\\n        model=\"gpt-4o-mini\",\\n    )\\n\\n\\u200b5. Trace OpenAI calls\\nThe first thing you might want to trace is all your OpenAI calls. LangSmith makes this easy with the wrap_openai (Python) or wrapOpenAI (TypeScript) wrappers. All you have to do is modify your code to use the wrapped client instead of using the OpenAI client directly.\\nPythonTypeScriptCopyAsk AIfrom openai import OpenAI\\nfrom langsmith.wrappers import wrap_openai\\n\\nopenai_client = wrap_openai(OpenAI())\\n\\n# This is the retriever we will use in RAG\\n# This is mocked out, but it could be anything we want\\ndef retriever(query: str):\\n    results = [\"Harrison worked at Kensho\"]\\n    return results\\n\\n# This is the end-to-end RAG chain.\\n# It does a retrieval step then calls OpenAI\\ndef rag(question):\\n    docs = retriever(question)\\n    system_message = \"\"\"Answer the users question using only the provided information below:\\n        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))\\n        \\n    return openai_client.chat.completions.create(\\n        messages=[\\n            {\"role\": \"system\", \"content\": system_message},\\n            {\"role\": \"user\", \"content\": question},\\n        ],\\n        model=\"gpt-4o-mini\",\\n    )\\n\\nNow when you call your application as follows:\\nCopyAsk AIrag(\"where did harrison work\")\\n\\nThis will produce a trace of just the OpenAI call in LangSmith’s default tracing project. It should look something like this.\\n\\n\\u200b6. Trace entire application\\nYou can also use the traceable decorator (Python or TypeScript) to trace your entire application instead of just the LLM calls.\\nPythonTypeScriptCopyAsk AIfrom openai import OpenAI\\nfrom langsmith import traceable\\nfrom langsmith.wrappers import wrap_openai\\n\\nopenai_client = wrap_openai(OpenAI())\\n\\ndef retriever(query: str):\\n    results = [\"Harrison worked at Kensho\"]\\n    return results\\n\\n@traceable\\ndef rag(question):\\n    docs = retriever(question)\\n    system_message = \"\"\"Answer the users question using only the provided information below:\\n        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))\\n        \\n    return openai_client.chat.completions.create(\\n        messages=[\\n            {\"role\": \"system\", \"content\": system_message},\\n            {\"role\": \"user\", \"content\": question},\\n        ],\\n        model=\"gpt-4o-mini\",\\n    )\\n\\nNow if you call your application as follows:\\nCopyAsk AIrag(\"where did harrison work\")\\n\\nThis will produce a trace of just the entire pipeline (with the OpenAI call as a child run) - it should look something like this\\n\\n\\u200bNext steps\\nCongratulations! If you’ve made it this far, you’re well on your way to being an expert in observability with LangSmith. Here are some topics you might want to explore next:\\n\\nTracing Integrations\\nSend traces to a specific project\\nFilter traces in a project\\n\\nIf you prefer a video tutorial, check out the Tracing Basics video from the Introduction to LangSmith Course.OverviewEvaluate an applicationAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs =loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d4007a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Data --> Docs -->Divide our text into chunks --> vectors --> Vector Embeddings --> Vector Store DB\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fd5ba60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='Tracing quickstart - Docs by LangChainOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KLangSmithPlatform for LLM observability and evaluationOverviewQuickstartsTrace an applicationEvaluate an applicationTest promptsAPI & SDKsAPI referencePython SDKJS/TS SDKPricingPlansPricing FAQOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KAsk AIForumForumSearch...NavigationQuickstartsTracing quickstartGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationForumOn this pageGet started1. Install Dependencies2. Create an API key3. Set up environment variables4. Define your application5. Trace OpenAI calls6. Trace entire applicationNext stepsQuickstartsTracing quickstartCopy pageCopy pageObservability is important for any software'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='up environment variables4. Define your application5. Trace OpenAI calls6. Trace entire applicationNext stepsQuickstartsTracing quickstartCopy pageCopy pageObservability is important for any software application, but especially so for LLM applications. LLMs are non-deterministic by nature, meaning they can produce unexpected results. This makes them trickier than normal to debug.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='This is where LangSmith can help! LangSmith gives you visibility into each step your application takes when handling a request — helping you debug faster and gain confidence in your app. From prototyping to production, LangSmith has you covered with tracing,  filtering, charting, and alerting to keep your application reliable at scale.\\n\\u200bGet started\\nThis tutorial will show you how to instrument a simple RAG application that consists of a retrieval step to fetch data and an LLM call to OpenAI to answer the user question based on the data.\\nIf you’re building an application with LangChain or LangGraph, you can enable LangSmith tracing with a single environment variable.Get started by reading the guides for tracing with LangChain or tracing with LangGraph.\\n\\u200b1. Install Dependencies\\nPythonTypeScriptCopyAsk AIpip install -U langsmith openai'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='\\u200b2. Create an API key\\nTo create an API key head to the LangSmith settings page. Then click + API Key.\\n\\u200b3. Set up environment variables\\nThis example uses OpenAI, but you can adapt it to use any LLM provider.\\nIf you’re using Anthropic, use the Anthropic wrapper to trace your calls. For other providers, use the traceable wrapper.\\nCopyAsk AIexport LANGSMITH_TRACING=true\\nexport LANGSMITH_API_KEY=\"<your-langsmith-api-key>\"\\nexport OPENAI_API_KEY=\"<your-openai-api-key>\"\\n\\n\\u200b4. Define your application\\nWe will instrument a simple RAG application for this tutorial, but feel free to use your own code if you’d like - just make sure it has an LLM call!\\nApplication CodePythonTypeScriptCopyAsk AIfrom openai import OpenAI\\nopenai_client = OpenAI()\\n\\n# This is the retriever we will use in RAG\\n# This is mocked out, but it could be anything we want\\ndef retriever(query: str):\\n    results = [\"Harrison worked at Kensho\"]\\n    return results'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='# This is the retriever we will use in RAG\\n# This is mocked out, but it could be anything we want\\ndef retriever(query: str):\\n    results = [\"Harrison worked at Kensho\"]\\n    return results\\n\\n# This is the end-to-end RAG chain.\\n# It does a retrieval step then calls OpenAI\\ndef rag(question):\\n    docs = retriever(question)\\n    system_message = \"\"\"Answer the users question using only the provided information below:\\n        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))\\n        \\n    return openai_client.chat.completions.create(\\n        messages=[\\n            {\"role\": \"system\", \"content\": system_message},\\n            {\"role\": \"user\", \"content\": question},\\n        ],\\n        model=\"gpt-4o-mini\",\\n    )'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='\\u200b5. Trace OpenAI calls\\nThe first thing you might want to trace is all your OpenAI calls. LangSmith makes this easy with the wrap_openai (Python) or wrapOpenAI (TypeScript) wrappers. All you have to do is modify your code to use the wrapped client instead of using the OpenAI client directly.\\nPythonTypeScriptCopyAsk AIfrom openai import OpenAI\\nfrom langsmith.wrappers import wrap_openai\\n\\nopenai_client = wrap_openai(OpenAI())\\n\\n# This is the retriever we will use in RAG\\n# This is mocked out, but it could be anything we want\\ndef retriever(query: str):\\n    results = [\"Harrison worked at Kensho\"]\\n    return results'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='# This is the retriever we will use in RAG\\n# This is mocked out, but it could be anything we want\\ndef retriever(query: str):\\n    results = [\"Harrison worked at Kensho\"]\\n    return results\\n\\n# This is the end-to-end RAG chain.\\n# It does a retrieval step then calls OpenAI\\ndef rag(question):\\n    docs = retriever(question)\\n    system_message = \"\"\"Answer the users question using only the provided information below:\\n        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))\\n        \\n    return openai_client.chat.completions.create(\\n        messages=[\\n            {\"role\": \"system\", \"content\": system_message},\\n            {\"role\": \"user\", \"content\": question},\\n        ],\\n        model=\"gpt-4o-mini\",\\n    )\\n\\nNow when you call your application as follows:\\nCopyAsk AIrag(\"where did harrison work\")\\n\\nThis will produce a trace of just the OpenAI call in LangSmith’s default tracing project. It should look something like this.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='This will produce a trace of just the OpenAI call in LangSmith’s default tracing project. It should look something like this.\\n\\n\\u200b6. Trace entire application\\nYou can also use the traceable decorator (Python or TypeScript) to trace your entire application instead of just the LLM calls.\\nPythonTypeScriptCopyAsk AIfrom openai import OpenAI\\nfrom langsmith import traceable\\nfrom langsmith.wrappers import wrap_openai\\n\\nopenai_client = wrap_openai(OpenAI())\\n\\ndef retriever(query: str):\\n    results = [\"Harrison worked at Kensho\"]\\n    return results\\n\\n@traceable\\ndef rag(question):\\n    docs = retriever(question)\\n    system_message = \"\"\"Answer the users question using only the provided information below:\\n        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))\\n        \\n    return openai_client.chat.completions.create(\\n        messages=[\\n            {\"role\": \"system\", \"content\": system_message},\\n            {\"role\": \"user\", \"content\": question},\\n        ],\\n        model=\"gpt-4o-mini\",\\n    )'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='Now if you call your application as follows:\\nCopyAsk AIrag(\"where did harrison work\")\\n\\nThis will produce a trace of just the entire pipeline (with the OpenAI call as a child run) - it should look something like this\\n\\n\\u200bNext steps\\nCongratulations! If you’ve made it this far, you’re well on your way to being an expert in observability with LangSmith. Here are some topics you might want to explore next:\\n\\nTracing Integrations\\nSend traces to a specific project\\nFilter traces in a project\\n\\nIf you prefer a video tutorial, check out the Tracing Basics video from the Introduction to LangSmith Course.OverviewEvaluate an applicationAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e94cec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"PPLX_API_KEY\" not in os.environ:\n",
    "    os.environ[\"PPLX_API_KEY\"] = getpass.getpass(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "295b5abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\",google_api_key=os.environ['GEMINI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3d956a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstoredb = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f70a06f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x251519d90a0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a076da59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\u200b2. Create an API key\\nTo create an API key head to the LangSmith settings page. Then click + API Key.\\n\\u200b3. Set up environment variables\\nThis example uses OpenAI, but you can adapt it to use any LLM provider.\\nIf you’re using Anthropic, use the Anthropic wrapper to trace your calls. For other providers, use the traceable wrapper.\\nCopyAsk AIexport LANGSMITH_TRACING=true\\nexport LANGSMITH_API_KEY=\"<your-langsmith-api-key>\"\\nexport OPENAI_API_KEY=\"<your-openai-api-key>\"\\n\\n\\u200b4. Define your application\\nWe will instrument a simple RAG application for this tutorial, but feel free to use your own code if you’d like - just make sure it has an LLM call!\\nApplication CodePythonTypeScriptCopyAsk AIfrom openai import OpenAI\\nopenai_client = OpenAI()\\n\\n# This is the retriever we will use in RAG\\n# This is mocked out, but it could be anything we want\\ndef retriever(query: str):\\n    results = [\"Harrison worked at Kensho\"]\\n    return results'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Query from a vector db\n",
    "\n",
    "query = \"This example uses OpenAI, but you can adapt it to use any LLM provider. If you’re using Anthropic\"\n",
    "result = vectorstoredb.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "50c7c76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_perplexity import ChatPerplexity\n",
    "llm = ChatPerplexity(model=\"sonar-pro\", temperature=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "46a165d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n    Answer the following question based on the provided context:\\n    <context>\\n    {context}\\n    </context>\\n    '), additional_kwargs={})])\n",
       "| ChatPerplexity(client=<openai.OpenAI object at 0x0000025121073170>, model='sonar-pro', temperature=0.0, model_kwargs={}, pplx_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Retrieval Chain, Document chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the following question based on the provided context:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b4fc4b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You can adapt the example that uses **OpenAI** to work with **Anthropic** by leveraging Anthropic’s Model Context Protocol (MCP), which provides a standardized, interoperable way to connect AI models to external tools and resources[1][2]. This involves switching from OpenAI’s centralized API approach to Anthropic’s protocol-focused, client-server architecture.\\n\\nTo adapt your integration:\\n\\n- **Replace OpenAI-specific API calls** with MCP-compatible requests. Anthropic’s MCP allows you to connect your LLM client (which could be OpenAI, Anthropic, or another provider) to MCP servers that expose tools and resources[2].\\n- **Initiate an MCP client** and connect it to the relevant MCP server (e.g., for database access or other tools)[2].\\n- **Load tools and resources** from the MCP server, converting them into function-calling formats compatible with your LLM (often using JSON Schema)[2].\\n- **Customize system messages** and prompts to reflect the available MCP features and tools[2].\\n- **Start your application loop** to handle user inputs and route them through the MCP infrastructure[2].\\n\\n**Key differences to consider:**\\n- **OpenAI’s approach** is centralized and product-focused, emphasizing ease of use and tight integration with their models and tools[1][5].\\n- **Anthropic’s MCP** is decentralized and protocol-focused, prioritizing interoperability and standardization, which may require more technical setup but offers greater flexibility for connecting various models and tools[1][2][5].\\n\\nIf you are switching requests from OpenAI to Anthropic, you may also need to implement routing logic to redirect API calls, handle quota or latency thresholds, and ensure compatibility with Anthropic’s API endpoints[3].\\n\\nIn summary, adapting from OpenAI to Anthropic involves shifting from direct API calls to a protocol-based integration using MCP, which enables broader interoperability but may require more technical configuration[1][2][5].'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document \n",
    "document_chain.invoke({\n",
    "    \"input\":\"This example uses OpenAI, but you can adapt it to use any LLM provider. If you’re using Anthropic\",\n",
    "    \"context\": [Document(page_content=\"This example uses OpenAI, but you can adapt it to use any LLM provider. If you’re using Anthropic\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2006e8d6",
   "metadata": {},
   "source": [
    "However, we want the documents to first come from the retriever we just set up. That way, we can use the retreiver to dynamically select the most relevent documents and pass those in for a given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8396425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x251519d90a0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Input --->Retreival ---> VectorStoreDB\n",
    "\n",
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0408777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever =vectorstoredb.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain = create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "79f52033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000251519D90A0>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n    Answer the following question based on the provided context:\\n    <context>\\n    {context}\\n    </context>\\n    '), additional_kwargs={})])\n",
       "            | ChatPerplexity(client=<openai.OpenAI object at 0x0000025121073170>, model='sonar-pro', temperature=0.0, model_kwargs={}, pplx_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "47b9f199",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the response from the LLM\n",
    "response =retrieval_chain.invoke({\n",
    "    \"input\":\"This example uses OpenAI, but you can adapt it to use any LLM provider. If you’re using Anthropic\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3449c812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To trace OpenAI calls in your RAG application using LangSmith, you need to wrap your OpenAI client with LangSmith’s tracing wrapper and set the appropriate environment variables, including your LangSmith API key.\\n\\n**Key steps:**\\n\\n- **Create an API key:** Go to the LangSmith settings page and generate an API key[5][1].\\n- **Set environment variables:**  \\n  - For recent LangSmith setups, use:\\n    ```\\n    export LANGSMITH_TRACING=true\\n    export LANGSMITH_API_KEY=\"<your-langsmith-api-key>\"\\n    export OPENAI_API_KEY=\"<your-openai-api-key>\"\\n    ```\\n  - Note: In older documentation, the variable was called `LANGCHAIN_API_KEY`, but for new projects, use `LANGSMITH_API_KEY`[3].\\n- **Wrap your OpenAI client:**  \\n  - Instead of using the OpenAI client directly, import and use the LangSmith wrapper:\\n    ```python\\n    from openai import OpenAI\\n    from langsmith.wrappers import wrap_openai\\n\\n    openai_client = wrap_openai(OpenAI())\\n    ```\\n  - This ensures all OpenAI calls are traced and visible in the LangSmith dashboard[5].\\n\\n**Summary of environment variables:**\\n\\n| Variable                | Purpose                                 | Example value                  |\\n|-------------------------|-----------------------------------------|-------------------------------|\\n| LANGSMITH_TRACING       | Enables tracing                         | true                          |\\n| LANGSMITH_API_KEY       | Authenticates with LangSmith            | <your-langsmith-api-key>      |\\n| OPENAI_API_KEY          | Authenticates with OpenAI               | <your-openai-api-key>         |\\n\\n**Additional notes:**\\n- You only need the `LANGSMITH_API_KEY` for new LangSmith projects; do not use the old `LANGCHAIN_API_KEY` unless working with legacy setups[3].\\n- Once set up, all LLM calls (like those in your RAG chain) will be automatically traced and viewable in the LangSmith UI[2][4].\\n\\nThis setup allows you to monitor, debug, and analyze your LLM application’s behavior directly from the LangSmith dashboard.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "625b5d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'This example uses OpenAI, but you can adapt it to use any LLM provider. If you’re using Anthropic',\n",
       " 'context': [Document(id='22ff76a5-1574-40ec-bcef-1b95a3d3d92e', metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='\\u200b2. Create an API key\\nTo create an API key head to the LangSmith settings page. Then click + API Key.\\n\\u200b3. Set up environment variables\\nThis example uses OpenAI, but you can adapt it to use any LLM provider.\\nIf you’re using Anthropic, use the Anthropic wrapper to trace your calls. For other providers, use the traceable wrapper.\\nCopyAsk AIexport LANGSMITH_TRACING=true\\nexport LANGSMITH_API_KEY=\"<your-langsmith-api-key>\"\\nexport OPENAI_API_KEY=\"<your-openai-api-key>\"\\n\\n\\u200b4. Define your application\\nWe will instrument a simple RAG application for this tutorial, but feel free to use your own code if you’d like - just make sure it has an LLM call!\\nApplication CodePythonTypeScriptCopyAsk AIfrom openai import OpenAI\\nopenai_client = OpenAI()\\n\\n# This is the retriever we will use in RAG\\n# This is mocked out, but it could be anything we want\\ndef retriever(query: str):\\n    results = [\"Harrison worked at Kensho\"]\\n    return results'),\n",
       "  Document(id='eeaa91dc-677c-44c3-8be6-8ed85ae3048f', metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='# This is the retriever we will use in RAG\\n# This is mocked out, but it could be anything we want\\ndef retriever(query: str):\\n    results = [\"Harrison worked at Kensho\"]\\n    return results\\n\\n# This is the end-to-end RAG chain.\\n# It does a retrieval step then calls OpenAI\\ndef rag(question):\\n    docs = retriever(question)\\n    system_message = \"\"\"Answer the users question using only the provided information below:\\n        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))\\n        \\n    return openai_client.chat.completions.create(\\n        messages=[\\n            {\"role\": \"system\", \"content\": system_message},\\n            {\"role\": \"user\", \"content\": question},\\n        ],\\n        model=\"gpt-4o-mini\",\\n    )'),\n",
       "  Document(id='c1b1e46c-0be9-4903-9282-28190eb1a96b', metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='# This is the retriever we will use in RAG\\n# This is mocked out, but it could be anything we want\\ndef retriever(query: str):\\n    results = [\"Harrison worked at Kensho\"]\\n    return results\\n\\n# This is the end-to-end RAG chain.\\n# It does a retrieval step then calls OpenAI\\ndef rag(question):\\n    docs = retriever(question)\\n    system_message = \"\"\"Answer the users question using only the provided information below:\\n        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))\\n        \\n    return openai_client.chat.completions.create(\\n        messages=[\\n            {\"role\": \"system\", \"content\": system_message},\\n            {\"role\": \"user\", \"content\": question},\\n        ],\\n        model=\"gpt-4o-mini\",\\n    )\\n\\nNow when you call your application as follows:\\nCopyAsk AIrag(\"where did harrison work\")\\n\\nThis will produce a trace of just the OpenAI call in LangSmith’s default tracing project. It should look something like this.'),\n",
       "  Document(id='38dff075-d7cf-4788-897e-535ce1c845b9', metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='\\u200b5. Trace OpenAI calls\\nThe first thing you might want to trace is all your OpenAI calls. LangSmith makes this easy with the wrap_openai (Python) or wrapOpenAI (TypeScript) wrappers. All you have to do is modify your code to use the wrapped client instead of using the OpenAI client directly.\\nPythonTypeScriptCopyAsk AIfrom openai import OpenAI\\nfrom langsmith.wrappers import wrap_openai\\n\\nopenai_client = wrap_openai(OpenAI())\\n\\n# This is the retriever we will use in RAG\\n# This is mocked out, but it could be anything we want\\ndef retriever(query: str):\\n    results = [\"Harrison worked at Kensho\"]\\n    return results')],\n",
       " 'answer': 'To trace OpenAI calls in your RAG application using LangSmith, you need to wrap your OpenAI client with LangSmith’s tracing wrapper and set the appropriate environment variables, including your LangSmith API key.\\n\\n**Key steps:**\\n\\n- **Create an API key:** Go to the LangSmith settings page and generate an API key[5][1].\\n- **Set environment variables:**  \\n  - For recent LangSmith setups, use:\\n    ```\\n    export LANGSMITH_TRACING=true\\n    export LANGSMITH_API_KEY=\"<your-langsmith-api-key>\"\\n    export OPENAI_API_KEY=\"<your-openai-api-key>\"\\n    ```\\n  - Note: In older documentation, the variable was called `LANGCHAIN_API_KEY`, but for new projects, use `LANGSMITH_API_KEY`[3].\\n- **Wrap your OpenAI client:**  \\n  - Instead of using the OpenAI client directly, import and use the LangSmith wrapper:\\n    ```python\\n    from openai import OpenAI\\n    from langsmith.wrappers import wrap_openai\\n\\n    openai_client = wrap_openai(OpenAI())\\n    ```\\n  - This ensures all OpenAI calls are traced and visible in the LangSmith dashboard[5].\\n\\n**Summary of environment variables:**\\n\\n| Variable                | Purpose                                 | Example value                  |\\n|-------------------------|-----------------------------------------|-------------------------------|\\n| LANGSMITH_TRACING       | Enables tracing                         | true                          |\\n| LANGSMITH_API_KEY       | Authenticates with LangSmith            | <your-langsmith-api-key>      |\\n| OPENAI_API_KEY          | Authenticates with OpenAI               | <your-openai-api-key>         |\\n\\n**Additional notes:**\\n- You only need the `LANGSMITH_API_KEY` for new LangSmith projects; do not use the old `LANGCHAIN_API_KEY` unless working with legacy setups[3].\\n- Once set up, all LLM calls (like those in your RAG chain) will be automatically traced and viewable in the LangSmith UI[2][4].\\n\\nThis setup allows you to monitor, debug, and analyze your LLM application’s behavior directly from the LangSmith dashboard.'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "adebd679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='22ff76a5-1574-40ec-bcef-1b95a3d3d92e', metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='\\u200b2. Create an API key\\nTo create an API key head to the LangSmith settings page. Then click + API Key.\\n\\u200b3. Set up environment variables\\nThis example uses OpenAI, but you can adapt it to use any LLM provider.\\nIf you’re using Anthropic, use the Anthropic wrapper to trace your calls. For other providers, use the traceable wrapper.\\nCopyAsk AIexport LANGSMITH_TRACING=true\\nexport LANGSMITH_API_KEY=\"<your-langsmith-api-key>\"\\nexport OPENAI_API_KEY=\"<your-openai-api-key>\"\\n\\n\\u200b4. Define your application\\nWe will instrument a simple RAG application for this tutorial, but feel free to use your own code if you’d like - just make sure it has an LLM call!\\nApplication CodePythonTypeScriptCopyAsk AIfrom openai import OpenAI\\nopenai_client = OpenAI()\\n\\n# This is the retriever we will use in RAG\\n# This is mocked out, but it could be anything we want\\ndef retriever(query: str):\\n    results = [\"Harrison worked at Kensho\"]\\n    return results'),\n",
       " Document(id='eeaa91dc-677c-44c3-8be6-8ed85ae3048f', metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='# This is the retriever we will use in RAG\\n# This is mocked out, but it could be anything we want\\ndef retriever(query: str):\\n    results = [\"Harrison worked at Kensho\"]\\n    return results\\n\\n# This is the end-to-end RAG chain.\\n# It does a retrieval step then calls OpenAI\\ndef rag(question):\\n    docs = retriever(question)\\n    system_message = \"\"\"Answer the users question using only the provided information below:\\n        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))\\n        \\n    return openai_client.chat.completions.create(\\n        messages=[\\n            {\"role\": \"system\", \"content\": system_message},\\n            {\"role\": \"user\", \"content\": question},\\n        ],\\n        model=\"gpt-4o-mini\",\\n    )'),\n",
       " Document(id='c1b1e46c-0be9-4903-9282-28190eb1a96b', metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='# This is the retriever we will use in RAG\\n# This is mocked out, but it could be anything we want\\ndef retriever(query: str):\\n    results = [\"Harrison worked at Kensho\"]\\n    return results\\n\\n# This is the end-to-end RAG chain.\\n# It does a retrieval step then calls OpenAI\\ndef rag(question):\\n    docs = retriever(question)\\n    system_message = \"\"\"Answer the users question using only the provided information below:\\n        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))\\n        \\n    return openai_client.chat.completions.create(\\n        messages=[\\n            {\"role\": \"system\", \"content\": system_message},\\n            {\"role\": \"user\", \"content\": question},\\n        ],\\n        model=\"gpt-4o-mini\",\\n    )\\n\\nNow when you call your application as follows:\\nCopyAsk AIrag(\"where did harrison work\")\\n\\nThis will produce a trace of just the OpenAI call in LangSmith’s default tracing project. It should look something like this.'),\n",
       " Document(id='38dff075-d7cf-4788-897e-535ce1c845b9', metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='\\u200b5. Trace OpenAI calls\\nThe first thing you might want to trace is all your OpenAI calls. LangSmith makes this easy with the wrap_openai (Python) or wrapOpenAI (TypeScript) wrappers. All you have to do is modify your code to use the wrapped client instead of using the OpenAI client directly.\\nPythonTypeScriptCopyAsk AIfrom openai import OpenAI\\nfrom langsmith.wrappers import wrap_openai\\n\\nopenai_client = wrap_openai(OpenAI())\\n\\n# This is the retriever we will use in RAG\\n# This is mocked out, but it could be anything we want\\ndef retriever(query: str):\\n    results = [\"Harrison worked at Kensho\"]\\n    return results')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5347482",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
