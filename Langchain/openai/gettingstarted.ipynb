{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2b3ab17",
   "metadata": {},
   "source": [
    "### LangChain\n",
    "- Get setup with LangChain, LangSmith and LangServe\n",
    "- Use the most basic and common components of LangChain: prompt templates, models, and output parsers.\n",
    "- Build a simple application with Langchain\n",
    "- Trace your application with LangSmith\n",
    "- Serve your application with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36d51001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"PPLX_API_KEY\"] = os.getenv('PPLX_API_KEY')\n",
    "# Langsmit tracking\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfb8857c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.OpenAI object at 0x000001B7C6E5AEA0> model_kwargs={} pplx_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_perplexity import ChatPerplexity\n",
    "llm = ChatPerplexity() \n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ba9bb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input and get response from LLM\n",
    "result = llm.invoke(\"What is generative AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19276120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='**Generative AI** is a type of artificial intelligence designed to create new, original content such as text, images, videos, audio, or software code based on user prompts. Unlike traditional AI, which might analyze or classify data, generative AI systems learn patterns and structures from large datasets and use this understanding to generate novel outputs that resemble the training data but are not direct copies[1][2][5].\\n\\nGenerative AI typically relies on advanced machine learning models, especially deep learning neural networks like large language models (LLMs) for text (e.g., ChatGPT), generative adversarial networks (GANs), and variational autoencoders (VAEs). These models are trained on vast amounts of data and learn to produce statistically probable new content by encoding and decoding information, often improving through feedback loops[4][5].\\n\\nThis technology has been widely adopted across many sectors, including healthcare, finance, entertainment, software development, and marketing, due to its ability to automate content creation and enhance productivity. However, generative AI also raises ethical, legal, and environmental concerns, such as potential misuse for misinformation, intellectual property issues, job displacement, and high energy consumption for training and operation[2][5].\\n\\nIn summary, generative AI is a powerful AI subset focused on creating new and original digital content by learning from existing data patterns, enabling applications from chatbots to image generation[1][3][5].' additional_kwargs={'citations': ['https://www.nnlm.gov/guides/data-thesaurus/generative-artificial-intelligence', 'https://www.ibm.com/think/topics/generative-ai', 'https://www.coursera.org/articles/what-is-generative-ai', 'https://education.illinois.edu/about/news-events/news/article/2024/11/11/what-is-generative-ai-vs-ai', 'https://en.wikipedia.org/wiki/Generative_artificial_intelligence'], 'search_results': [{'title': 'Generative Artificial Intelligence | NNLM', 'url': 'https://www.nnlm.gov/guides/data-thesaurus/generative-artificial-intelligence', 'date': '2024-02-26', 'last_updated': '2025-08-29', 'snippet': 'Generative Artificial Intelligence (AI) is a system of algorithms or computer processes that can create novel output in text, images or other media based on ...'}, {'title': 'What is Generative AI? - IBM', 'url': 'https://www.ibm.com/think/topics/generative-ai', 'date': '2024-03-22', 'last_updated': '2025-08-29', 'snippet': 'Generative AI excels at analyzing large datasets, identifying patterns and extracting meaningful insights, then generating hypotheses and recommendations based on those insights to support executives, analysts, researchers and other professionals in making smarter, data-driven decisions.'}, {'title': 'What Is Generative AI? How It Works, Examples, Benefits ... - Coursera', 'url': 'https://www.coursera.org/articles/what-is-generative-ai', 'date': '2025-08-15', 'last_updated': '2025-08-29', 'snippet': 'Generative AI is a type of artificial intelligence tool that generates images, text, videos, and other media in response to inputted prompts ...'}, {'title': \"Traditional AI vs. Generative AI: What's the Difference? | Illinois\", 'url': 'https://education.illinois.edu/about/news-events/news/article/2024/11/11/what-is-generative-ai-vs-ai', 'date': '2024-11-11', 'last_updated': '2025-08-29', 'snippet': 'Generative AI is used to generate information—thus, it is used in content creation and design and in scientific research, where scientists are ...'}, {'title': 'Generative artificial intelligence - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/Generative_artificial_intelligence', 'date': '2023-03-14', 'last_updated': '2025-08-29', 'snippet': 'Generative artificial intelligence is a subfield of artificial intelligence that uses generative models to produce text, images, videos, or other forms of ...'}]} response_metadata={'model_name': 'sonar'} id='run--11e6aea6-b2b4-47e6-8839-fbd23fec99b3-0' usage_metadata={'input_tokens': 6, 'output_tokens': 287, 'total_tokens': 293}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5d96687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer, Provide me answer based on the questions'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Chatprompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer, Provide me answer based on the questions\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f20a27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='**LangSmith** is a dedicated platform and framework developed by the team behind LangChain, designed to **monitor, debug, evaluate, and observe applications built with large language models (LLMs)**. It provides developers with visibility and traceability into the inner workings of LLMs and AI agents within their products, helping to identify why AI systems may fail or behave unexpectedly, especially in production environments[2][3].\\n\\nKey features of LangSmith include:\\n\\n- **Tracing:** It records detailed \"traces\" of AI decisions and interactions, analogous to logs in programming, showing inputs, outputs, and internal reasoning steps of LLM chains and agents[2].\\n- **Debugging:** Tools to investigate complex issues like slow or looping agent workflows, prompt problems, or tool and memory usage errors, enabling faster diagnosis and fixes[2][3].\\n- **Performance Monitoring:** Developers can test different prompt versions and monitor real-time performance to ensure reliability[3].\\n- **Integration:** LangSmith integrates seamlessly with LangChain, a popular Python framework that lets developers build complex LLM applications by chaining models, agents, and memory components[3][4].\\n\\nLangSmith is part of the broader LangChain ecosystem, supporting the entire AI engineering lifecycle—from prototyping to production—helping developers ship reliable AI apps faster[1].\\n\\nAdditional benefits highlighted include its ease of use, flexibility to build diverse LLM-powered applications (chatbots, content generators, machine translation), and scalability for demanding use cases[5].\\n\\nIn summary, LangSmith is an observability and debugging platform that makes building and maintaining production-ready LLM applications more transparent, robust, and manageable for developers[2][3][4].' additional_kwargs={'citations': ['https://www.langchain.com/about', 'https://command.ai/blog/langsmith/', 'https://www.ibm.com/think/topics/langsmith', 'https://www.youtube.com/watch?v=Iyc80hY2yYk', 'https://www.bluebash.co/blog/artificial-intelligence-meet-langsmith/'], 'search_results': [{'title': 'About', 'url': 'https://www.langchain.com/about', 'date': '2024-01-01', 'last_updated': '2025-08-29', 'snippet': 'We help developers make the impossible, possible. LangChain is the platform developers and enterprises choose to build AI apps from prototype to production.'}, {'title': 'How we use LangSmith to analyze millions of customer chats', 'url': 'https://command.ai/blog/langsmith/', 'date': '2023-09-19', 'last_updated': '2025-08-29', 'snippet': \"LangSmith is a framework built on the shoulders of LangChain. It's designed to track the inner workings of LLMs and AI agents within your product.\"}, {'title': 'What is LangSmith?', 'url': 'https://www.ibm.com/think/topics/langsmith', 'date': '2025-06-12', 'last_updated': '2025-08-29', 'snippet': 'LangSmith serves as a dedicated platform for monitoring, debugging and evaluating applications built with large language models.'}, {'title': 'LangSmith 101 for AI Observability | Full Walkthrough', 'url': 'https://www.youtube.com/watch?v=Iyc80hY2yYk', 'date': '2025-06-30', 'last_updated': '2025-08-22', 'snippet': 'LangSmith is a built-in observability service and platform that integrates easily with LangChain. We use LangSmith as an incredibly powerful ...'}, {'title': 'Meet LangSmith Transforming Large Language Model App', 'url': 'https://www.bluebash.co/blog/artificial-intelligence-meet-langsmith/', 'date': '2023-08-26', 'last_updated': '2025-08-27', 'snippet': 'LangChain has simplified the development of applications powered by large language models, making them accessible to a wider range of users ...'}]} response_metadata={'model_name': 'sonar'} id='run--8ab2f874-29ba-4f79-84ee-a9fb3fbb209c-0' usage_metadata={'input_tokens': 22, 'output_tokens': 338, 'total_tokens': 360}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt|llm\n",
    "\n",
    "response = chain.invoke({\"input\" : \"can you tell me about LangSmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d399f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88ee2836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**LangSmith** is a platform developed by the team behind LangChain that provides tools for **monitoring, debugging, and evaluating applications built with large language models (LLMs)**. It is designed to give developers full visibility and control over the inner workings of LLMs and AI agents within their products, helping to track inputs, outputs, and decision-making processes at every step in the AI workflow[2][3][4].\n",
      "\n",
      "Key features and purposes of LangSmith include:\n",
      "\n",
      "- **Debugging:** It helps developers identify and fix issues such as unexpected results, errors, slow performance, or problematic prompt designs by providing detailed traces (logs) of the AI's internal operations. These traces show what text was input and output at each step, illuminating the model's thought process and actions[2][4].\n",
      "\n",
      "- **Performance Monitoring:** LangSmith exposes latency and token usage metrics, allowing teams to pinpoint which API calls or model interactions are causing delays or inefficiencies[4].\n",
      "\n",
      "- **Evaluation and Testing:** Developers can test different prompt versions and rerun examples directly from the LangSmith user interface, reducing friction in prompt engineering and iteration[4].\n",
      "\n",
      "- **Integration with LangChain:** LangSmith complements LangChain, an open-source Python framework for building LLM applications with modular building blocks like chains, agents, and memory. While LangChain focuses on creating the workflows and applications, LangSmith provides the observability and debugging layer to make those applications more reliable and production-ready[1][3].\n",
      "\n",
      "LangSmith is widely adopted and has been used by companies like Snowflake and Streamlit to build intelligent, reliable AI agents. It supports major AI models including OpenAI and Anthropic, with plans to extend support further[4]. Additionally, it integrates with cloud platforms such as Google Cloud, AWS, and Microsoft Azure, facilitating scalable AI application development[5].\n",
      "\n",
      "In summary, **LangSmith is a unified platform designed to help AI developers bridge the gap between prototype and production by providing deep insight, debugging, and evaluation tools for LLM-powered applications**[4].\n"
     ]
    }
   ],
   "source": [
    "## String output parser\n",
    "from langchain_core.output_parsers import  StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt|llm|output_parser\n",
    "\n",
    "response = chain.invoke({\"input\" : \"can you tell me about LangSmith?\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cafc36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
